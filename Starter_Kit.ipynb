{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GollyTicker/Food-Recognition-Challenge/blob/main/Starter_Kit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt_yLzMyRHP8"
   },
   "source": [
    "![AIcrowd-Logo](https://raw.githubusercontent.com/AIcrowd/AIcrowd/master/app/assets/images/misc/aicrowd-horizontal.png)\n",
    "\n",
    "This dataset and notebook correspond to the [Food Recognition Challenge](https://www.aicrowd.com/challenges/food-recognition-challenge) being held on [AIcrowd](https://www.aicrowd.com/).\n",
    "\n",
    "<p align=\"right\"> Join the communty! <br><a href=\"https://discord.gg/GTckBMx\"><img src=\"https://img.shields.io/discord/657211973435392011?style=for-the-badge\" alt=\"chat on Discord\"></a>\n",
    "</p>\n",
    "\n",
    "# <center> üçï Food Recognition Challenge: Detectron2 starter kit </center>\n",
    "\n",
    "<center>This notebook aims to build a model for food detection and segmentation using <code>detectron2</code></center>\n",
    "\n",
    "# How to use this notebook? üìù\n",
    "1. **Copy the notebook**. This is a shared template and any edits you make here will not be saved. _You should copy it into your own drive folder._ For this, click the \"File\" menu (top-left), then \"Save a Copy in Drive\". You can edit your copy however you like.\n",
    "2. **Make a submission**. Run all the code in the notebook to get a feel of how the notebook and the submission process works.\n",
    "3. **Try tweaking the parameters**. If you are new to the problem, a great way to start is try tweaking the configuration flags, train your model and submit again.\n",
    "4. **Diving into the code**. When you submit via this notebook, we create a repository on [gitlab.aicrowd.com](https://gitlab.aicrowd.com). You can check the code we generated based on this notebook and directly make changes you want there!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqCwgApzqFK6"
   },
   "source": [
    "# Setup the notebook üõ†\n",
    "\n",
    "If you are running this locally (instead of in Colab), then please use Conda and execute the setup instructions in estup-windows.txt. After that, you can continue with these setup instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_embMQfcqIK5",
    "outputId": "ca43b21b-2b2d-449d-d169-f1e64268c1f5"
   },
   "outputs": [],
   "source": [
    "# !bash <(curl -sL https://gitlab.aicrowd.com/jyotish/food-recognition-challenge-detectron2-baseline/raw/master/utils/setup-colab.sh)\n",
    "!pip install -q numpy python-dotenv minio multiprocess scikit-image nbdime\n",
    "!pip install -q cython aicrowd-api Pillow opencv-python\n",
    "!pip install -q tornado~=5.1.0\n",
    "# !pip install -f https://download.pytorch.org/whl/cu101/torch_stable.html torch==1.5 torchvision==0.6\n",
    "# !pip install -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html detectron2==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFZHYrf_SvIx",
    "outputId": "a8761139-b284-4da3-86d1-afd89990e924"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IeBrALmMbWY",
    "outputId": "bc77e4c2-ca20-4710-e4b3-db576a31b37f"
   },
   "outputs": [],
   "source": [
    "!pip install -qq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9ExFnHGTEYL"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "def path_exists(path):\n",
    "  return Path(path).exists()\n",
    "\n",
    "def wget(url):\n",
    "  dest = url.split('/')[-1]\n",
    "  if path_exists(dest):\n",
    "    print(f\"{dest} already exists.\")\n",
    "  else:\n",
    "    print(\"Downloading\",url,end=\" \")\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "def remove_recursive_force(dir):\n",
    "  shutil.rmtree(dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qME8zdhWTMWk",
    "outputId": "90bac448-266c-421b-e357-0081bc74d8fb"
   },
   "outputs": [],
   "source": [
    "wget(\"https://raw.githubusercontent.com/GollyTicker/Food-Recognition-Challenge/main/poolstore.py\")\n",
    "print(\"INFO: poolstore.py is taken from master and may be inconsistent with the revision of the currently checked-out notebook.\")\n",
    "remove_recursive_force(\"pool-store\")\n",
    "os.makedirs(\"pool-store\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JveWajFEMfCR",
    "outputId": "fc0e30e1-597d-4467-e1cc-ea3cb2818fb2"
   },
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "import os\n",
    "import json\n",
    "from multiprocess import Pool # Attention: We use multiprocess instead of multiprocessing library.\n",
    "# This works for parallelism on Windows in Jupyter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "import platform\n",
    "import skimage\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from poolstore import *\n",
    "\n",
    "from fastbook import *\n",
    "#hide\n",
    "# test gpu is used\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "# assert torch.cuda.is_available()\n",
    "\n",
    "WINDOWS = platform.system() == 'Windows'\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "PAR_PROCS = 3 if WINDOWS else multiprocess.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rfdXyyASvIz"
   },
   "source": [
    "##### Parallel Processing\n",
    "`multiprocessing` had execution issues on Windows within jupyter. Hence, this notebook uses `multiprocess`. However, to ensure, that all dependencies (variables) are useable in the worker task, I created a script `poolstore` which is imported in the cells here and in the worker task. The variables need to be stored to disk via `storePool(myVar, \"myVar\")` and restored in the worker function via `loadPool(\"myVar\", globals())`. **Changes in global variables are not reflected across worker processes and the main process.**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "cf2ec2a22bd54d4abcabcfd0dfa2f33f",
      "fbe7be75d1e144deb6ac161d5d2e92ae",
      "8ef6530ba41d4ea6843b7446cb869a03",
      "d90e19e725ea409f958c67b9cb5581bd",
      "fc09ee524b6d430ea618dfe20ac8c2e1",
      "d56c205969074c2b949491975071a9ff",
      "f5b54b0ac00a422db1bac3fc2dd5a171",
      "e9c92eab54994e72b9418c02411f2220",
      "d7d24f6f8aaf4a899fa829ceceb059a0",
      "f9ea918c2fad469b8b03ee49768912e4",
      "3ae740682d564544a849983cdf18f4f5"
     ]
    },
    "id": "X6XY2wceSvI0",
    "outputId": "5e2b331d-4fd4-4ab4-ff7c-5c4805984392"
   },
   "outputs": [],
   "source": [
    "def prepareTask(taskVarName):\n",
    "  storePool(globals()[taskVarName], \"currentTask\")\n",
    "  def worker(*args):\n",
    "    from poolstore import loadPool\n",
    "    loadPool(\"currentTask\", globals())\n",
    "    return currentTask(*args)\n",
    "  return worker\n",
    "\n",
    "data = np.random.random((2,10))\n",
    "constdata = np.random.random((10))\n",
    "constdata2 = np.random.random((10))\n",
    "\n",
    "def subfunction(a, b):\n",
    "  return a * b + constdata2\n",
    "\n",
    "def exampleTask(xs):\n",
    "  return subfunction(xs, constdata)[:3]\n",
    "  \n",
    "def doStuff(data):\n",
    "  result = []\n",
    "  \n",
    "  f = prepareTask(\"exampleTask\")\n",
    "\n",
    "  with Pool(PAR_PROCS) as p:\n",
    "    with tqdm(total=len(data)) as progress_bar:\n",
    "      for funcres in p.imap(f, data):\n",
    "        result.append(funcres)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "  return result\n",
    "\n",
    "if np.array(doStuff(data)).shape == (2,3):\n",
    "  print(\"Parallel processing works.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USdOZipySvI1"
   },
   "source": [
    "#### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHpLohCmSvI1"
   },
   "outputs": [],
   "source": [
    "if WINDOWS:\n",
    "  urllib.request.urlretrieve(\"https://github.com/stedolan/jq/releases/download/jq-1.6/jq-win64.exe\", \"jq.exe\")\n",
    "else:\n",
    "  os.system(\"wget -q -O jq https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 && chmod +x jq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9TA_gz38qjn"
   },
   "outputs": [],
   "source": [
    "#!wget -q -nv https://datasets.aicrowd.com/default/aicrowd-public-datasets/food-recognition-challenge/v0.4/train-v0.4.tar.gz\n",
    "#!wget -q -nv https://datasets.aicrowd.com/default/aicrowd-public-datasets/food-recognition-challenge/v0.4/val-v0.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAouH2oI8Kic",
    "outputId": "9a726611-005e-4c5a-cf61-f499531b9dc7"
   },
   "outputs": [],
   "source": [
    "wget(\"https://s3.eu-central-1.wasabisys.com/aicrowd-public-datasets/myfoodrepo/round-1/train.tar.gz\")\n",
    "wget(\"https://s3.eu-central-1.wasabisys.com/aicrowd-public-datasets/myfoodrepo/round-1/test_images.tar.gz\")\n",
    "wget(\"https://s3.eu-central-1.wasabisys.com/aicrowd-public-datasets/myfoodrepo/round-1/val.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXO6-8_-9Fye",
    "outputId": "18115176-0979-4f4b-85ee-1b1e39dedb60"
   },
   "outputs": [],
   "source": [
    "def ls(path, n=10000):\n",
    "  for f in os.listdir(Path(path))[:n]:\n",
    "    print(f)\n",
    "\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "ls(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBqcZYsgSvI4",
    "outputId": "707c78c4-c03d-4822-be3a-d967fa93d8c2"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def untar_idempotent(file):\n",
    "  ext = \".tar.gz\"\n",
    "  assert file.endswith(ext)\n",
    "  filename = file[:-len(ext)]\n",
    "  if path_exists(filename):\n",
    "    print(file,\"already extracted.\")\n",
    "  else:\n",
    "    with tarfile.open(file, \"r:gz\") as tar:\n",
    "      tar.extractall()\n",
    "    print(file,\"extracted.\")\n",
    "\n",
    "for file in os.listdir(\".\"):\n",
    "  if file.endswith('.tar.gz'):\n",
    "    untar_idempotent(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-OseIO9-DAD",
    "outputId": "23a8fca8-e17f-48ad-d50c-19a015c818da"
   },
   "outputs": [],
   "source": [
    "ls(\"train\")\n",
    "\n",
    "if WINDOWS:\n",
    "  print(\"Preview of annotations.json not on Windows.\")\n",
    "else:\n",
    "  os.system(\"cat train/annotations.json | ./jq . | head -n 50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDA1bgkg-8yu"
   },
   "source": [
    "# Configure static variables üìé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4MJxXiHvQHn"
   },
   "source": [
    "On, Colab, if you are using a S3 storage to store models, intermediate data etc., then you can point to it here.\n",
    "\n",
    "If it does not exist, then the notebook will always recompute stuff instead of caching them on the S3.\n",
    "\n",
    "If you are executing this locally, you do not need this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPSrviCIyiJZ"
   },
   "outputs": [],
   "source": [
    "# The credentials for the storage as expected to be read from this file in Google Drive.\n",
    "# You can populate that file using:\n",
    "\n",
    "# echo \"USER=<user>\" >> \"/content/gdrive/My Drive/<path-to-file>\"\n",
    "# echo \"PASSWORD=<password>\" >> \"/content/gdrive/My Drive/<path-to-file>\"\n",
    "# echo \"URL=<endpoint-url>\" >> \"/content/gdrive/My Drive/<path-to-file>\"\n",
    "# echo \"PATH_WITH_BUCKET=some-bucket/some/path/in/bucket/without/trailing/slash\" >> \"/content/gdrive/My Drive/<path-to-file>\"\n",
    "\n",
    "STORAGE_CREDENTIALS_PATH = Path(\"/content/gdrive/My Drive/swnt-storage-credentials.txt\") if not WINDOWS else Path(\"swnt-storage-credentials.txt\")\n",
    "\n",
    "STORAGE = {\n",
    "  \"USER\": None,\n",
    "  \"PASSWORD\": None,\n",
    "  \"URL\": None,\n",
    "  \"BUCKET\": None,\n",
    "  \"PATH\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pGX4C6UtVBV",
    "outputId": "d03ed9c3-cb00-41c7-ab68-9fcdf81ee54d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from minio import Minio, S3Error\n",
    "from minio.deleteobjects import DeleteObject\n",
    "STORAGE_AVAILABLE=False\n",
    "\n",
    "storage_client = None\n",
    "\n",
    "# Note: All invocations of storage_client need to explicitly map between platform dependent paths\n",
    "# and s3 key (always posix). This should happen only directly at client call sites\n",
    "def to_s3key(path):\n",
    "  \"a\\\\b\\\\c.txt and a/b/c.txt -> to a/b/c.txt\"\n",
    "  return str(Path(path).as_posix())\n",
    "\n",
    "def from_s3key(key):\n",
    "  \"a/b/c.txt -> platform-dependent path\"\n",
    "  return Path(key)\n",
    "\n",
    "def extract_storage_credentials():\n",
    "  credentials = dotenv_values(STORAGE_CREDENTIALS_PATH)\n",
    "  \n",
    "  try:\n",
    "    STORAGE[\"USER\"] = credentials[\"USER\"]\n",
    "    STORAGE[\"PASSWORD\"] = credentials[\"PASSWORD\"]\n",
    "    STORAGE[\"URL\"] = credentials[\"URL\"]\n",
    "  \n",
    "    path_with_bucket = credentials[\"PATH_WITH_BUCKET\"]\n",
    "    STORAGE[\"BUCKET\"] = path_with_bucket.split(\"/\")[0]\n",
    "    STORAGE[\"PATH\"] = Path(path_with_bucket[len(STORAGE[\"BUCKET\"])+1:])\n",
    "\n",
    "    print(\"Found storage credentials with user\", STORAGE[\"USER\"],\"at\", STORAGE[\"URL\"])\n",
    "  \n",
    "  except KeyError as e:\n",
    "    print(f\"Please ensure USER, PASSWORD and URL are defined in the file {STORAGE_CREDENTIALS_PATH}?\")\n",
    "    raise e\n",
    "\n",
    "def make_storage_client():\n",
    "  global storage_client\n",
    "  storage_client = Minio(\n",
    "      STORAGE[\"URL\"],\n",
    "      access_key = STORAGE[\"USER\"],\n",
    "      secret_key = STORAGE[\"PASSWORD\"]\n",
    "  )\n",
    "\n",
    "def storage_list_objects_with_prefix(prefix):\n",
    "  \"\"\"Attention! You need to use from_s3key(error[i].object_name) to retrieve the platform dependent path\"\"\"\n",
    "  prefix = to_s3key(prefix)\n",
    "  return storage_client.list_objects(STORAGE[\"BUCKET\"], prefix=prefix, recursive=True)\n",
    "\n",
    "def storage_remove_all_with_prefix(prefix):\n",
    "  prefix = to_s3key(prefix)\n",
    "  test_objects_to_delete = storage_list_objects_with_prefix(prefix)\n",
    "  delete_objects_list = map(lambda obj: DeleteObject(obj.object_name), test_objects_to_delete)\n",
    "  errors = storage_client.remove_objects(STORAGE[\"BUCKET\"], delete_objects_list)\n",
    "  return errors\n",
    "\n",
    "def storage_remove_object_idempotent(key):\n",
    "  key = to_s3key(key)\n",
    "  try:\n",
    "    storage_client.remove_object(STORAGE[\"BUCKET\"], key)\n",
    "  except S3Error as e:\n",
    "    if e.code != \"NoSuchKey\":\n",
    "      raise e\n",
    "\n",
    "def storage_upload_single_file(file, key, metadata=None):\n",
    "  file_size = os.stat(file).st_size\n",
    "  with open(file, \"rb\") as f:\n",
    "    storage_upload_bytestream(key, f, length=file_size, metadata=metadata)\n",
    "\n",
    "def storage_upload_bytestream(key, stream, length, metadata=None, debug=False):\n",
    "    key = to_s3key(key)\n",
    "    if debug:\n",
    "      print(f\"Storing blob of length {length} at {key}\")\n",
    "    storage_client.put_object(STORAGE[\"BUCKET\"], key, stream, length=length, metadata=metadata)\n",
    "    \n",
    "def silently(func):\n",
    "  try:\n",
    "    func()\n",
    "  except Exception as e:\n",
    "    pass\n",
    "\n",
    "def storage_get_object_response_action(key, action):\n",
    "  key = to_s3key(key)\n",
    "  try:\n",
    "      response = storage_client.get_object(STORAGE[\"BUCKET\"], key)\n",
    "      return action(response)\n",
    "  finally:\n",
    "      silently(lambda: response.close())\n",
    "      silently(lambda: response.release_conn())\n",
    "\n",
    "def storage_get_object_as_blob(key):\n",
    "  key = to_s3key(key)\n",
    "  return storage_get_object_response_action(key, lambda response: response.data)\n",
    "      \n",
    "def storage_object_exists(key):\n",
    "  key = to_s3key(key)\n",
    "  try:\n",
    "    storage_client.stat_object(STORAGE[\"BUCKET\"], key)\n",
    "    return True\n",
    "  except S3Error as e:\n",
    "    if e.code == \"NoSuchKey\":\n",
    "      return False\n",
    "    else:\n",
    "      raise e\n",
    "  \n",
    "def test_storage_with_files():\n",
    "  test_suffix = str(random.randint(100000,999999))\n",
    "  test_prefix = STORAGE['PATH'] / \"delete-me---storage-access-test-tmp-\"\n",
    "  test_path = f\"{test_prefix}-{test_suffix}\"\n",
    "  test_content = b\"sdlfslife\"\n",
    "  \n",
    "  storage_upload_bytestream(test_path, io.BytesIO(test_content), len(test_content))\n",
    "\n",
    "  result = storage_get_object_as_blob(test_path)\n",
    "\n",
    "  assert test_content == result\n",
    "\n",
    "  errors = list( storage_remove_all_with_prefix(test_prefix) )\n",
    "  \n",
    "  for err in errors:\n",
    "    print(\"Error while deleting:\", err)\n",
    "  assert len(errors) == 0\n",
    "\n",
    "  print(\"Probing of storage successful.\")\n",
    "\n",
    "def test_storage():\n",
    "  try:\n",
    "    make_storage_client()\n",
    "    test_storage_with_files()\n",
    "  except Exception as e:\n",
    "    print(\"Failed to write, read or delete a test object in storage.\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "if STORAGE_CREDENTIALS_PATH.exists():\n",
    "  extract_storage_credentials()\n",
    "\n",
    "  test_storage()\n",
    "\n",
    "  STORAGE_AVAILABLE=True\n",
    "\n",
    "if STORAGE_AVAILABLE:\n",
    "  print(f\"STORAGE_AVAILABLE = {STORAGE_AVAILABLE}. Some functions will cache results.\")\n",
    "else:\n",
    "  print(f\"STORAGE_AVAILABLE = {STORAGE_AVAILABLE}. All computed results will be lost when your Colab is closed!\")\n",
    "  print(\"To store and cache results, please specify a S3 compatible storage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83P8YszC_qV5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "STORAGE_MIRRORS = STORAGE[\"PATH\"] / \"mirrors\"\n",
    "\n",
    "def storage_store_directory_idempotent(dir, name, debug=False):\n",
    "  \"Mirrors the given directory onto the storage. If run a second time, the backup already exists and it is then not overwritten.\"\n",
    "  mirrorRoot = to_s3key(STORAGE_MIRRORS / name)\n",
    "\n",
    "  for _ in storage_list_objects_with_prefix(mirrorRoot):\n",
    "    print(f\"Mirrors: Name {name} is already defined and will not be stored again.\")\n",
    "    return\n",
    "\n",
    "  print(f\"Storing directory for name {name}.\")\n",
    "\n",
    "  for root, dirs, files in os.walk(dir):\n",
    "    for file in files:\n",
    "      relpath = Path(root) / file\n",
    "      if debug:\n",
    "        print(\"Save\",relpath,\" -> \",mirrorRoot/relpath)\n",
    "      storage_upload_single_file(file=relpath, key=mirrorRoot/relpath)\n",
    "\n",
    "def print_on_error(msg, errors):\n",
    "  for err in errors:\n",
    "    print(msg, err)\n",
    "\n",
    "def storage_remove_stored_directory(name):\n",
    "  print_on_error(\"Error removing\", storage_remove_all_with_prefix(STORAGE_MIRRORS / name))\n",
    "  print(f\"Removed stored {name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TN0ltyKDJchy",
    "outputId": "ca2dec72-bf64-4be7-89ee-e59271751a18"
   },
   "outputs": [],
   "source": [
    "def set_file_contents(file, bytes):\n",
    "  os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "  with open(file,\"wb+\") as f:\n",
    "    f.write(bytes)\n",
    "\n",
    "if \"test directories\":\n",
    "  d = Path(\"test-dir\")\n",
    "  \n",
    "  for dr in [d, d / \"1\", d / \"2\"]:\n",
    "    os.makedirs(dr, exist_ok=True)\n",
    "\n",
    "  set_file_contents(d/\"1\"/\"1.txt\", b\"1.1\")\n",
    "  set_file_contents(d/\"2\"/\"2.txt\", b\"2.2\")\n",
    "  set_file_contents(d/\"2\"/\"3.txt\", b\"2.3\")\n",
    "\n",
    "  if STORAGE_AVAILABLE:\n",
    "    storage_store_directory_idempotent(\"test-dir\", name=\"test\", debug=True)\n",
    "    storage_remove_stored_directory(name=\"test\")\n",
    "    remove_recursive_force(\"test-dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlT6e9C7SvI7"
   },
   "outputs": [],
   "source": [
    "STORAGE_ARTIFACTS = STORAGE[\"PATH\"] / \"artifacts\"\n",
    "\n",
    "def storage_store_artifact_by_name(artifact_relpath, name, force=False):\n",
    "  assert \"/\" not in str(name) and \"\\\\\" not in str(name), \"Names may not contain any / or \\\\.\"\n",
    "  \n",
    "  path = to_s3key(STORAGE_ARTIFACTS / name)\n",
    "  normalised_relpath = to_s3key(artifact_relpath)\n",
    "\n",
    "  exists = storage_object_exists(path)\n",
    "  \n",
    "  save_file = lambda: storage_upload_single_file(file=artifact_relpath, key=path, metadata = {\"relpath\": normalised_relpath})\n",
    "  \n",
    "  if exists and not force:\n",
    "    print(f\"Artifact with name {name} is already defined and will not be stored again.\")\n",
    "    return\n",
    "  elif exists and force:\n",
    "    save_file()\n",
    "    print(f\"Artifact with name {name} was overwritten in storage at {path}.\")\n",
    "  else:\n",
    "    save_file()\n",
    "    print(f\"Stored artifact for name {name} at {path}.\")\n",
    "    \n",
    "def storage_retrieve_artifact_by_name(name):\n",
    "  blob, normalised_relpath = storage_get_object_response_action(\n",
    "    key= STORAGE_ARTIFACTS / name,\n",
    "    action = lambda response: (response.data, response.headers.get(\"x-amz-meta-relpath\", None)) # x-amz-meta-\n",
    "  )\n",
    "  if normalised_relpath is None:\n",
    "    raise ValueError(f\"Stored artifacts must have metadata relpath defined! Name {name}\")\n",
    "    \n",
    "  relpath = from_s3key(normalised_relpath)\n",
    "  set_file_contents(relpath, blob)\n",
    "  print(f\"Restored artifact with name {name} to {relpath}.\")\n",
    "    \n",
    "def storage_list_artifacts():\n",
    "  return [ from_s3key(obj.object_name).name for obj in storage_list_objects_with_prefix(STORAGE_ARTIFACTS) ]\n",
    "\n",
    "def storage_remove_artifact_idempotent(name):\n",
    "  storage_remove_object_idempotent(STORAGE_ARTIFACTS / name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hij28HO8SvI8",
    "outputId": "13f4c8a9-c9b2-4368-9227-7e027a36a0bd"
   },
   "outputs": [],
   "source": [
    "# artifact storage test\n",
    "\n",
    "if \"artifact test\":\n",
    "  test_path = Path(\"file-test\")/\"path.txt\"\n",
    "  os.makedirs(\"file-test\", exist_ok=True)\n",
    "  contents = b\"abcdef\"\n",
    "  set_file_contents(test_path, contents)\n",
    "\n",
    "  storage_store_artifact_by_name(test_path, \"test-file-1\", force=True)\n",
    "  \n",
    "  assert \"path.txt\" in list(os.listdir(\"file-test\"))\n",
    "  remove_recursive_force(\"file-test\")\n",
    "  assert \"file-test\" not in list(os.listdir(\".\"))\n",
    "  \n",
    "  storage_retrieve_artifact_by_name(\"test-file-1\")\n",
    "  assert \"path.txt\" in list(os.listdir(\"file-test\"))\n",
    "  with open(test_path,\"rb\") as f:\n",
    "    assert f.read() == contents\n",
    "    \n",
    "  assert \"test-file-1\" in storage_list_artifacts()\n",
    "  storage_remove_artifact_idempotent(\"test-file-1\")\n",
    "  assert \"test-file-1\" not in storage_list_artifacts()\n",
    "\n",
    "  remove_recursive_force(\"file-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zc4ZRDB7LbzM"
   },
   "outputs": [],
   "source": [
    "# todo. continue here\n",
    "if STORAGE_AVAILABLE:\n",
    "  pass # storage_load_directory_overwrite_local(\"test-dir\", name=\"test\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6kEtMOl-8GO"
   },
   "outputs": [],
   "source": [
    "class Paths:\n",
    "  DATASET_DIR = Path(\".\")\n",
    "  \n",
    "  TRAIN_DATA_DIR = DATASET_DIR/\"train\"\n",
    "  TRAIN_IMAGES_DIR = TRAIN_DATA_DIR/\"images\"\n",
    "  TRAIN_ANNOTATIONS = TRAIN_DATA_DIR/\"annotations.json\"\n",
    "  TRAIN_MULTI_SEG_DIR = TRAIN_DATA_DIR/\"segmentations\"\n",
    "  \n",
    "  VAL_DATA_DIR = DATASET_DIR/\"val\"\n",
    "  VAL_ANNOTATIONS = VAL_DATA_DIR/\"annotations.json\"\n",
    "  VAL_IMAGES_DIR = VAL_DATA_DIR/\"images\"\n",
    "  VAL_MULTI_SEG_DIR = VAL_DATA_DIR/\"segmentations\"\n",
    "\n",
    "  CURR_DATA_DIR = DATASET_DIR/\"curr\"\n",
    "  CURR_IMAGES_DIR = CURR_DATA_DIR/\"images\"\n",
    "  CURR_ANNOTATIONS = CURR_DATA_DIR/\"annotations.json\"\n",
    "  CURR_MULTI_SEG_DIR = CURR_DATA_DIR/\"segmentations\"\n",
    "\n",
    "\n",
    "class DatasetLabels:\n",
    "  TRAIN = \"dataset_train\"\n",
    "  VAL = \"dataset_val\"\n",
    "\n",
    "remove_recursive_force(Paths.CURR_IMAGES_DIR)\n",
    "for dirs in [Paths.CURR_IMAGES_DIR,Paths.TRAIN_MULTI_SEG_DIR,Paths.VAL_MULTI_SEG_DIR,Paths.CURR_MULTI_SEG_DIR]:\n",
    "  os.makedirs(dirs,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jkAnlhzIvyM"
   },
   "outputs": [],
   "source": [
    "# this cell needs to be updated\n",
    "\n",
    "if False:\n",
    "  GDRIVE_MIRROR_ROOT = gdrive/\"Food Challenge\"/\"persistent\"/\"mirror\"\n",
    "  os.makedirs(GDRIVE_MIRROR_ROOT,exist_ok=True)\n",
    "\n",
    "  def store_directory_to_google_drive(relative_dir):\n",
    "    destination = GDRIVE_MIRROR_ROOT/relative_dir\n",
    "    print(f\"Storing {relative_dir} -> {destination} (target removed before copy)\")\n",
    "    shutil.rmtree(destination, ignore_errors=True)\n",
    "    shutil.copytree(relative_dir,destination)\n",
    "\n",
    "  def restore_directory_from_google_drive(relative_dir):\n",
    "    source = GDRIVE_MIRROR_ROOT/relative_dir\n",
    "    print(f\"Restoring {relative_dir} <- {source} (target removed before copy)\")\n",
    "    shutil.rmtree(relative_dir, ignore_errors=True)\n",
    "    shutil.copytree(source,relative_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpUFct7TK3Sj"
   },
   "outputs": [],
   "source": [
    "# restore precomputed segmentation masks\n",
    "if False and \"we don't want to restore the segmentation masks from google drive, since this is slow. It's slower than computing them directly!\":\n",
    "  try:\n",
    "    restore_directory_from_google_drive(Paths.TRAIN_MULTI_SEG_DIR)\n",
    "  except Exception as e:\n",
    "    e.printStackTrace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGt1_dNs_LFB"
   },
   "source": [
    "# Packages üóÉ\n",
    "\n",
    "Import here all the packages you need to define your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOxxkvKI5jhA"
   },
   "outputs": [],
   "source": [
    "\"\"\"from detectron2.data.datasets import register_coco_instances\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.events import get_event_storage\n",
    "from detectron2.engine import HookBase\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgc_ZDzmzyOE"
   },
   "source": [
    "# Loading the data üì≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xii1QDbCpZ0K",
    "outputId": "a5b35ec0-c3ac-41b6-9db7-a786d08aeb8c"
   },
   "outputs": [],
   "source": [
    "with open(Paths.TRAIN_ANNOTATIONS) as fp:\n",
    "  annotations = json.load(fp)\n",
    "  print(annotations.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7Yr5lfoAnwJ"
   },
   "source": [
    "## Helper functions to clean the dataset\n",
    "\n",
    "First, we will see if all the annotations in the dataset are properly aligned with the images. These helper functions will let us do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SstjAZ9UR6mk"
   },
   "outputs": [],
   "source": [
    "# LIMITATION: multiprocess does not seem to support global variables... all functions seem to need to be stand-alone...\n",
    "\n",
    "def validate_annotation(annotation):\n",
    "  \"\"\"Check the image dimensions and fix them if needed\n",
    "  \"\"\"\n",
    "  import os\n",
    "  filepath = annotation.get(\"file_name\")\n",
    "  if not os.path.exists(filepath):\n",
    "    return annotation, filepath\n",
    "  img = cv2.imread(filepath)\n",
    "  if img.shape[0] != annotation.get(\"height\") or img.shape[1] != annotation.get(\"width\"):\n",
    "    annotation[\"height\"], annotation[\"width\"] = annotation[\"width\"], annotation[\"height\"]\n",
    "  return annotation, None\n",
    "\n",
    "\n",
    "def clean_annotations(annotation_images, name):\n",
    "  \"\"\"Read the image dimensions and fix them in parallel\n",
    "  \"\"\"\n",
    "  annotated_images = []\n",
    "\n",
    "  with open(\"clean-\"+name+\".log\", \"w\") as f:\n",
    "\n",
    "    with Pool() as p:\n",
    "      total_images = len(annotation_images)\n",
    "\n",
    "      with tqdm(total=total_images) as progress_bar:\n",
    "\n",
    "        for annotation, filepath_if_skipped in p.imap(validate_annotation, annotation_images):\n",
    "          annotated_images.append(annotation)\n",
    "          \n",
    "          if filepath_if_skipped is not None:\n",
    "            f.write(\"Reusing precomputed: \" + filepath_if_skipped)\n",
    "\n",
    "          progress_bar.update(1)\n",
    "\n",
    "  return annotated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbxqgfj0Xd8Q"
   },
   "source": [
    "## Clean the training data üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "12092f96182f48ccb6a0699823f35b6e",
      "85ce7cdcc53544518a51a31bff93c81e",
      "0e0ec7d3fb2c40dd953992e3ffe94dc4",
      "e3f28e3ba2ec434bad2a98919e127f65",
      "1e7b97148f414244b351ea2cf1f93565",
      "24da74bd0f5047b997880eb274e13bdd",
      "6005fbc4e6be42c097cae12d6c03fafc",
      "aa0ef12cd9f543538b9ee0b2059e0ac3",
      "3a4db9db30bf43ee875bbd16a7b15732",
      "571d4530eb7748968c892eb0a65c1aaa",
      "999982caa21d4f6c825f778edfa073d5"
     ]
    },
    "id": "gvap0qHNUYzf",
    "outputId": "1cd03bbe-a813-412a-fb31-751a78e1fa23"
   },
   "outputs": [],
   "source": [
    "image_dir = Paths.TRAIN_IMAGES_DIR\n",
    "annotations[\"images\"] = clean_annotations(annotations.get(\"images\"), \"train\")\n",
    "\n",
    "with open(Paths.TRAIN_ANNOTATIONS, \"w\") as fp:\n",
    "  json.dump(annotations, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZM_-xKcXhvH"
   },
   "source": [
    "## Clean the validation data üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2fdf11aad98b4862950ac00a6d29e73e",
      "aab9d20dd4604a71a37b3c74626cb353",
      "6f75519591d244f69e857c870ca2fe84",
      "e5f280703daf409d850e371fda8204c0",
      "9ce9f7c87d3140748f15c11851547f04",
      "2c0bd818e72d4898b44f50c3446dee38",
      "e38318e2b9554cc48538095838f6f240",
      "54488d6fc9234ec7aa95837b7c645378",
      "a16f964148534988844916605e60db5d",
      "07dc319d05a04d16ae9925c649f09ff1",
      "28158f33ed2b4af49f8c2796a13d4c95"
     ]
    },
    "id": "0y1ADqjrV9Kn",
    "outputId": "258dd874-0913-45d7-c6e4-f2bb3d239254"
   },
   "outputs": [],
   "source": [
    "image_dir = Paths.VAL_IMAGES_DIR\n",
    "\n",
    "with open(Paths.VAL_ANNOTATIONS) as fp:\n",
    "  validation_annotations = json.load(fp)\n",
    "\n",
    "validation_annotations[\"images\"] = clean_annotations(validation_annotations.get(\"images\"), \"valid\")\n",
    "\n",
    "with open(Paths.VAL_ANNOTATIONS, \"w\") as fp:\n",
    "  json.dump(validation_annotations, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M05KHR--CUq3"
   },
   "source": [
    "# My baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfprvZ7ECWnA",
    "outputId": "f59d1f1d-bf44-4f9a-b894-d218f096d547"
   },
   "outputs": [],
   "source": [
    "coco = COCO(Paths.TRAIN_ANNOTATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9FRSJyRCo9-",
    "outputId": "f07e3468-705a-4d8c-be0d-98db12b3bc2f"
   },
   "outputs": [],
   "source": [
    "category_ids = coco.loadCats(coco.getCatIds())\n",
    "category_names = [_[\"name\"] for _ in category_ids]\n",
    "\n",
    "N_CLASSES = len(category_ids)\n",
    "\n",
    "image_ids = coco.getImgIds()\n",
    "\n",
    "annotation_ids = coco.getAnnIds()\n",
    "annotations = coco.loadAnns(annotation_ids)\n",
    "\n",
    "print(\"Categories: \",\", \".join(category_names))\n",
    "\n",
    "print(\"Image ids: \", image_ids[:10],\"...\")\n",
    "\n",
    "print(\"Annotation:\" , annotations[0].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNIYH15PSvJA"
   },
   "source": [
    "#### Choosing the subset of the data to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNpmnXTfWdUl"
   },
   "outputs": [],
   "source": [
    "N_ALL = 5545\n",
    "N = N_ALL\n",
    "choice = np.linspace(0,len(image_ids)-1,N,dtype=np.int16)\n",
    "tiny_train_imids = [image_ids[ci] for ci in choice]\n",
    "#tiny_train_imids, N = [12886,22949,49145,54634], 4\n",
    "assert len(tiny_train_imids) == N\n",
    "\n",
    "# TODO. we are splitting the train into train / val again. We could now use the aicrowd validation set as the test set.\n",
    "\n",
    "# create currently trained on dataset\n",
    "for imid in tiny_train_imids:\n",
    "  im = coco.loadImgs(imid)[0]\n",
    "  shutil.copy(Paths.TRAIN_IMAGES_DIR/im['file_name'], Paths.CURR_IMAGES_DIR/im['file_name'])\n",
    "  \n",
    "shutil.copy(Paths.TRAIN_ANNOTATIONS, Paths.CURR_ANNOTATIONS)\n",
    "\n",
    "\n",
    "testN = len(os.listdir(Paths.CURR_IMAGES_DIR))\n",
    "assert testN == N, f\"Expected {N} images in current training folder. But found {testN}. Please ensure, that you don't have cached images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9NEInrKS_Vo",
    "outputId": "621e0bc5-3fb8-411f-cd12-91655c532a97"
   },
   "outputs": [],
   "source": [
    "ls(\"curr\")\n",
    "ls(\"curr/images\", n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wvHBPWqa7aA",
    "outputId": "ebae6a9d-d3d2-4022-802e-300e72662c23"
   },
   "outputs": [],
   "source": [
    "[catdict[\"name\"] for catdict in category_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctgT-F9bYDqC",
    "outputId": "9294066c-f97f-4360-afe8-e7e40aa5c963"
   },
   "outputs": [],
   "source": [
    "SEG_DTYPE = np.float32\n",
    "\n",
    "fai_multi_cats_from_coco_ids = Pipeline([\n",
    "  MultiCategorize(vocab=[catdict[\"id\"] for catdict in category_ids]),\n",
    "  OneHotEncode(c=N_CLASSES),\n",
    "  Transform(enc = lambda tensor_mcat: np.array(tensor_mcat,dtype=SEG_DTYPE), dec = TensorMultiCategory, order=2)\n",
    "])\n",
    "\n",
    "coco_cat_id_to_name = lambda coco_cat_id : list(filter(lambda catdict: catdict[\"id\"] == coco_cat_id, category_ids))[0][\"name\"]\n",
    "fai_multi_cats_to_names = Transform(lambda fai_cats: list(map(coco_cat_id_to_name, fai_multi_cats_from_coco_ids.decode(fai_cats))))\n",
    "\n",
    "print(category_ids[0])\n",
    "print(category_ids[35])\n",
    "print(category_ids[24])\n",
    "\n",
    "print(fai_multi_cats_from_coco_ids)\n",
    "\n",
    "example_categories = fai_multi_cats_from_coco_ids([1565,2099,1468])\n",
    "print(\"bread-wholemeal,jam,rice ->\", example_categories)\n",
    "\n",
    "print(\"bread-wholemeal,jam,rice -> \", fai_multi_cats_to_names(example_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGa3OshczaT9"
   },
   "source": [
    "#### Analyse Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "id": "WvGv6Aa5zN37",
    "outputId": "42ae1ee8-4e6d-45ed-b944-d260f8264ff3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_frequencies = []\n",
    "\n",
    "def show_category_frequencies():\n",
    "  coco_categories_for_histogram = [coco_cat_id_to_name(anno[\"category_id\"]) for anno in annotations]\n",
    "\n",
    "  plt.figure(figsize=(8,12))\n",
    "  hist, _, _ = plt.hist(coco_categories_for_histogram, len(category_names), rwidth=0.5)\n",
    "  plt.ylabel(\"number of images with this category\")\n",
    "  plt.xticks(rotation=90)\n",
    "  global category_frequencies\n",
    "  category_frequencies = hist / hist.sum()\n",
    "  plt.show()\n",
    "\n",
    "show_category_frequencies()\n",
    "\n",
    "print(\"Category frequencies:\", category_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_axOw-KqSvJB"
   },
   "source": [
    "#### Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIMMhF0_qwxD"
   },
   "outputs": [],
   "source": [
    "if not \"example mini image masks\":\n",
    "  example_mini_image_masks = np.zeros((N_CLASSES,3,3),dtype=SEG_DTYPE)\n",
    "\n",
    "  example_mini_image_masks[2] = np.array([[1,1,1],[1,1,0],[1,0,0]])\n",
    "  example_mini_image_masks[5] = np.array([[1,0,0],[1,1,0],[1,1,1]])\n",
    "\n",
    "  print(example_mini_image_masks.shape)\n",
    "\n",
    "  example_mini_image_masks[:6]\n",
    "\n",
    "  for i in range(3):\n",
    "    for j in range(3):\n",
    "      print(f\"{i},{j} -> \",fai_multi_cats_from_coco_ids.decode(example_mini_image_masks[:,i,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "bhB3IQroC6xW",
    "outputId": "f3ec9d70-5973-40bd-87e9-545b4344149e"
   },
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "imid = 30573\n",
    "imid = 16387\n",
    "imid = random.choice(image_ids)\n",
    "im = coco.loadImgs(imid)[0]\n",
    "im_annotations = coco.loadAnns(coco.getAnnIds(imgIds=imid))\n",
    "\n",
    "image_path = os.path.join(Paths.TRAIN_IMAGES_DIR, im[\"file_name\"])\n",
    "plt.figure(figsize=(8,8))\n",
    "I = io.imread(image_path)\n",
    "plt.imshow(I)\n",
    "plt.axis('off')\n",
    "# Render annotations on top of the image\n",
    "coco.showAnns(im_annotations)\n",
    "\n",
    "print(imid,im[\"width\"],im[\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ4_NW8_LdbG"
   },
   "outputs": [],
   "source": [
    "def one_hot_id_from_coco_id(coco_id):\n",
    "  return np.nonzero(fai_multi_cats_from_coco_ids([coco_id]))[0][0]\n",
    "\n",
    "def convert_annotations_to_multimask(im_annotations, image_dict, debug=False):\n",
    "  \"\"\"Produces a mask of shape [N_CLASSES, w, h] where each seg_pixel (c,x,y) is either 1 (in class) or 0\"\"\"\n",
    "\n",
    "  multi_seg_mask = np.zeros((N_CLASSES, image_dict[\"width\"], image_dict[\"height\"]),dtype=SEG_DTYPE)\n",
    "\n",
    "  if debug:\n",
    "    print(\"Image id\", image_dict['id'])\n",
    "    print(\"Image shape: \", image_dict['width'], image_dict['height'])\n",
    "\n",
    "  for im_anno in im_annotations:\n",
    "    encSeg = cocomask.frPyObjects(\n",
    "        im_anno['segmentation']\n",
    "        ,image_dict['width'], image_dict['height']\n",
    "      )\n",
    "    \n",
    "    one_hot_id = one_hot_id_from_coco_id(im_anno[\"category_id\"])\n",
    "    \n",
    "    if debug:\n",
    "      print(\"Processing\",im_anno['category_id'],\"->\",one_hot_id)\n",
    "    \n",
    "    org_seg = cocomask.decode(encSeg)\n",
    "\n",
    "    if debug:\n",
    "      print(\"Org seg shape \", org_seg.shape)\n",
    "      print(\"Number of components of this label \", org_seg.shape[2])\n",
    "\n",
    "    multi_component_segmentations = org_seg.reshape((image_dict['width'], image_dict['height'],-1))\n",
    "\n",
    "    if debug:\n",
    "      print(\"multi_component_segmentations.shape \", multi_component_segmentations.shape)\n",
    "\n",
    "    category_seg = np.sum(multi_component_segmentations, axis=2)\n",
    "    np.clip(category_seg,0,1,out=category_seg)\n",
    "\n",
    "    assert category_seg.shape == (image_dict[\"width\"],image_dict[\"height\"])\n",
    "\n",
    "    multi_seg_mask[one_hot_id] = category_seg\n",
    "\n",
    "  return multi_seg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TkHCOTMZHsa-",
    "outputId": "3fd3c7da-2da4-4130-9ed1-7eb1aa57554d"
   },
   "outputs": [],
   "source": [
    "# test multi-label segmentation is correctly converted\n",
    "\n",
    "import pycocotools\n",
    "import pycocotools.mask as cocomask\n",
    "\n",
    "result = convert_annotations_to_multimask(im_annotations,im,debug=True)\n",
    "\n",
    "assert result.shape == (40, im[\"width\"], im[\"height\"])\n",
    "\n",
    "for im_anno in im_annotations:\n",
    "  encSeg = cocomask.frPyObjects(\n",
    "      im_anno['segmentation']\n",
    "      ,im['width'], im['height']\n",
    "    )\n",
    "  \n",
    "  one_hot_id = one_hot_id_from_coco_id(im_anno['category_id'])\n",
    "\n",
    "  m = cocomask.decode(encSeg)\n",
    "\n",
    "  m = m.reshape((im['width'], im['height'], -1))\n",
    "  for i in range(m.shape[2]):\n",
    "    print(\"component =\",i)\n",
    "    plt.imshow(m[:,:,i])\n",
    "    plt.show()\n",
    "  \n",
    "  print(\"mask\")\n",
    "  plt.imshow(result[one_hot_id])\n",
    "  plt.show()\n",
    "\n",
    "  assert np.all(m.sum(axis=2) == result[one_hot_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKRB5Xzg4QyF"
   },
   "outputs": [],
   "source": [
    "# Transform given annotations into our desired segmentations format\n",
    "\n",
    "FORCE_RECOMPUTE = False\n",
    "\n",
    "dest_segmentations_dir = None\n",
    "\n",
    "def needs_recomputation(im_id):\n",
    "  out_file = dest_segmentations_dir / f\"{im_id}.npz\"\n",
    "\n",
    "  return FORCE_RECOMPUTE or not os.path.isfile(out_file)\n",
    "\n",
    "def task(im_id):\n",
    "  try:\n",
    "    out_file = dest_segmentations_dir / f\"{im_id}.npz\"\n",
    "    image_dict = coco.loadImgs(im_id)[0]\n",
    "    im_annotations = coco.loadAnns(coco.getAnnIds(imgIds=im_id))\n",
    "    multi_mask = convert_annotations_to_multimask(im_annotations, image_dict, debug=False)\n",
    "    np.savez_compressed(out_file, multi_mask, allow_pickle=False)\n",
    "  except Exception as e:\n",
    "    print(\"Error: \", im_id)\n",
    "    raise e\n",
    "\n",
    "def preprocess_and_store_segmentations(im_ids):\n",
    "  worker = prepareTask(\"task\")\n",
    "  inputs = [id for id in im_ids if needs_recomputation(id)]\n",
    "  \n",
    "  print(f\"{len(inputs)} of {len(im_ids)} need segmentations to be recomputed.\")\n",
    "  with Pool(int(PAR_PROCS*3/2)) as p:\n",
    "    with tqdm(total=len(im_ids)) as progress_bar:\n",
    "      for _ in p.imap(worker, inputs):\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbYH7cc-51f9"
   },
   "outputs": [],
   "source": [
    "def load_multi_segmentation(path):\n",
    "  arr = None\n",
    "  with np.load(path) as npz:\n",
    "    arr = npz[\"arr_0\"]\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "9c6e6f779f484d98937e23c7b0e166f6",
      "770da34f27954335889fedcc42d91582",
      "d7b2553bb2504f17a54b16b75ec29080",
      "c1aff1a21d4f40158d5bba895058a107",
      "f4bdecbf9b9f45e3b33e5d4877cb3925",
      "b52424e01f7f46c79e92357bb7c8dc6b",
      "455ef386d22c4544a6156aadb97f46c0",
      "c83ecb46088041f1ad283b24064be2b9",
      "5611b8eb887c4d92a68c7cc484f9aaa4",
      "0d32ceb4ff484eafaa599eea9f682366",
      "447d90aef0ab49b8a0eabe9a77264b55"
     ]
    },
    "id": "Zt6BQzXn9hnQ",
    "outputId": "86a94984-1cd2-4fe2-e16c-a299ee6db5ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dest_segmentations_dir = Paths.CURR_MULTI_SEG_DIR\n",
    "preprocess_and_store_segmentations(tiny_train_imids)\n",
    "print(list(os.listdir(Paths.CURR_MULTI_SEG_DIR))[:5],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzyKyDmLSvJD"
   },
   "outputs": [],
   "source": [
    "if not \"transform train data\":\n",
    "  dest_segmentations_dir = Paths.TRAIN_MULTI_SEG_DIR\n",
    "  preprocess_and_store_segmentations(image_ids)\n",
    "  print(list(os.listdir(Paths.TRAIN_MULTI_SEG_DIR))[:5],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PKFTAdAKQTk"
   },
   "outputs": [],
   "source": [
    "if FORCE_RECOMPUTE and not \"we will not store it to google drive due ot it's slow read speed\":\n",
    "  store_directory_to_google_drive(Paths.TRAIN_MULTI_SEG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkiJEfRJnNzo"
   },
   "outputs": [],
   "source": [
    "def segmentation_size_fraction(seg):\n",
    "  assert np.all(np.logical_or(seg == 0, seg == 1))\n",
    "  assert seg.shape == (seg.shape[0],seg.shape[1])\n",
    "  segSize = np.count_nonzero(seg)\n",
    "  return float(segSize) / seg.size\n",
    "\n",
    "def resize_mseg(mseg, debug=False):\n",
    "  dest = np.zeros((N_CLASSES, IM_SZ,IM_SZ), dtype=SEG_DTYPE)\n",
    "  for i in range(N_CLASSES):\n",
    "    dest_seg_shape = (IM_SZ, IM_SZ)\n",
    "    has_defined_segmentation_for_class = np.any(mseg[i] == 1)\n",
    "    if not has_defined_segmentation_for_class:\n",
    "      continue\n",
    "    \n",
    "    asimg = PIL.Image.fromarray(mseg[i]) # using autodetected PIL Image Modes: https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes\n",
    "    # Pillow Image resize: https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.resize\n",
    "    resized = asimg.resize(dest_seg_shape, resample=PIL.Image.NEAREST)\n",
    "    resized = (np.array(resized, dtype=SEG_DTYPE) > 0.5).astype(SEG_DTYPE)\n",
    "\n",
    "    if debug:\n",
    "      print(\"original\",i)\n",
    "      plt.imshow(mseg[i])\n",
    "      plt.show()\n",
    "      print(\"resized\",i)\n",
    "      plt.imshow(resized)\n",
    "      plt.show()\n",
    "\n",
    "    assert resized.shape == dest_seg_shape\n",
    "    assert np.all(np.logical_or(resized == 0,resized == 1))\n",
    "\n",
    "    if \"check fraction of segmentation is approx. equal\":\n",
    "      org_frac = segmentation_size_fraction(mseg[i])\n",
    "      new_frac = segmentation_size_fraction(resized)\n",
    "      abs_diff = np.abs(new_frac - org_frac)\n",
    "      assert abs_diff < 0.01, f\"Segmentation size fraction has changed during resizing. i {i}, original {org_frac}, new {new_frac}\"\n",
    "    \n",
    "    dest[i] = resized\n",
    "\n",
    "  return dest\n",
    "  \n",
    "if not \"show transformed segmentations for data\":\n",
    "  for fname in fnames:\n",
    "    print(f\"{Path(fname).stem}\")\n",
    "    mseg = load_multi_segmentation(Paths.CURR_MULTI_SEG_DIR/f\"{Path(str(int(fname.stem)))}.npz\")\n",
    "    resize_mseg(mseg, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zH0CpAri5p8",
    "outputId": "326ccfe8-6e0a-4572-882e-2b81bc2beadf"
   },
   "outputs": [],
   "source": [
    "ls(\"curr/images\",n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAI9EG4viqsO",
    "outputId": "96116d99-c9ee-4973-de6e-a6e36895c4d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls(\"curr/segmentations\",n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvEQbHtASvJE"
   },
   "source": [
    "### fastai DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuV-1-NkPZ0m",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fastai delegation decorator: https://www.fast.ai/2019/08/06/delegation/\n",
    "\n",
    "SEED=1337\n",
    "# How to write a data block: https://docs.fast.ai/tutorial.datablock.html#Building-a-DataBlock-from-scratch\n",
    "class MultiLabelSegmentationDataLoaders(DataLoaders):\n",
    "    @classmethod\n",
    "    @delegates(DataLoaders.from_dblock)\n",
    "    def from_label_func(cls, path, fnames, label_func, valid_pct=0.3, seed=None, codes=None, item_tfms=None, batch_tfms=None, **kwargs):\n",
    "        dblock = DataBlock(blocks=(ImageBlock, TransformBlock),\n",
    "                           splitter=RandomSplitter(valid_pct, seed=seed),\n",
    "                           get_y=label_func,\n",
    "                           item_tfms=item_tfms,\n",
    "                           batch_tfms=batch_tfms)\n",
    "        res = cls.from_dblock(dblock, fnames, path=path, **kwargs)\n",
    "        return res\n",
    "\n",
    "IM_SZ = 256\n",
    "resize_args = (IM_SZ, fastbook.ResizeMethod.Squish)\n",
    "\n",
    "fnames = get_image_files(Paths.CURR_IMAGES_DIR)\n",
    "\n",
    "BATCH_SZ = min(N // 2, 5) # NOTE. Batch size is small due to GPU size limitations\n",
    "dls = MultiLabelSegmentationDataLoaders.from_label_func(\n",
    "  Paths.CURR_DATA_DIR, bs=BATCH_SZ, fnames = fnames,\n",
    "  label_func = lambda o: resize_mseg(load_multi_segmentation(Paths.CURR_MULTI_SEG_DIR/f\"{int(o.stem)}.npz\")),\n",
    "  codes = category_ids,\n",
    "  item_tfms=Resize(*resize_args),\n",
    "  seed = SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwhseLVVVAvo",
    "outputId": "36a6a9cd-b3bc-471f-f073-a8083d05345d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"train\",dls.train.n,dls.train.n_subsets)\n",
    "\n",
    "print(\"valid\",dls.valid.n,dls.valid.n_subsets)\n",
    "\n",
    "assert len(dls.train) >= 1, \"Training data generator is empty. Ensure, that the batch-size is small enough.\"\n",
    "assert len(dls.valid) >= 1, \"Validation data generator is empty. Ensure that the batch-size is small enough.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArXhndkaSvJF"
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "RmdsnG116X0M",
    "outputId": "413884f0-d3d2-4ec6-ea15-1f9b8beee957",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# category balancing for BCELogitsLoss and variants\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "\n",
    "category_weights = tensor((1 - category_frequencies) / category_frequencies, dtype=torch.float16)\n",
    "# e.g. category_frequency = [0.3, 0.7]\n",
    "# cat_rest_freq = [0.7, 0.3]\n",
    "# weight = [2.33, 0.428]\n",
    "category_weights_normalised = category_weights / category_weights.sum()\n",
    "\n",
    "plt.plot(category_weights)\n",
    "plt.show()\n",
    "plt.plot(category_weights_normalised)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rK1Vi56FYPW"
   },
   "outputs": [],
   "source": [
    "from enum import auto\n",
    "\n",
    "broadcasted_weights = category_weights_normalised.reshape((1,N_CLASSES,1,1)).broadcast_to((1,N_CLASSES,2,2))\n",
    "\n",
    "class LossType(Enum):\n",
    "  \n",
    "  CROSS_ENTROPY = (auto(),\n",
    "    torch.nn.CrossEntropyLoss(reduction='mean', weight=category_weights_normalised.to(device=DEVICE))\n",
    "  )\n",
    "  \n",
    "  MSE = (auto(),\n",
    "    lambda x, y: torch.mean(torch.linalg.vector_norm(x - y, dim=(1,2,3)))\n",
    "  )\n",
    "  \n",
    "  # MULTI_LABEL_MARGIN_LOSS = (auto(), torch.nn.MultiLabelMarginLoss(reduction='mean'))\n",
    "  \n",
    "  # ONLY USE THIS, IF NO SIGMOID IS IN THE LAST LAYER OF THE NETWORK ALREADY\n",
    "  BINARY_CROSS_ENTROPY_WITH_LOGITS_LOSS = (auto(),\n",
    "    torch.nn.BCEWithLogitsLoss(\n",
    "      reduction='mean',\n",
    "      pos_weight=category_weights.reshape((1,N_CLASSES,1,1)).to(device=DEVICE)\n",
    "        # alt: tensor[np.newaxis,N_CLASSES,np.newaxis,np.newaxis]\n",
    "        # Bug with pytorch in pos_weight: https://discuss.pytorch.org/t/loss-function-for-multi-class-semantic-segmentation/40596?u=ptrblck\n",
    "  ))\n",
    "  \n",
    "  # ONLY USE THIS IF THERE IS A SIGMOID IN THE LAST LAYER OF THE NETWORK\n",
    "  BINARY_CROSS_ENTROPY = (auto(),\n",
    "    torch.nn.BCELoss(\n",
    "      reduction='mean',\n",
    "      weight=category_weights_normalised.reshape((1,N_CLASSES,1,1)).to(device=DEVICE)\n",
    "    ))\n",
    "\n",
    "  def apply(self, x, y):\n",
    "    return self.value[1](x, y)\n",
    "\n",
    "USE_BCE = False\n",
    "\n",
    "LOSS_FUNCTION = LossType.BINARY_CROSS_ENTROPY if USE_BCE else LossType.BINARY_CROSS_ENTROPY_WITH_LOGITS_LOSS\n",
    "LOSS_MULTIPLIER = 1000 if USE_BCE else 1 # used to prevent the loss from becoming to small\n",
    "\n",
    "def loss_func(x,y):\n",
    "  assert x.shape == y.shape, f\"Expected equal shapes, but got: {x.shape} and {y.shape}\"\n",
    "  assert x.shape[1:] == (N_CLASSES, IM_SZ, IM_SZ), f\"Unexpected shape: {x.shape}\"\n",
    "  assert 1 <= x.shape[0] and x.shape[0] <= BATCH_SZ, f\"Unexpected shape (batch): {x.shape}\"\n",
    "  \n",
    "  result = LOSS_FUNCTION.apply(x, y) * LOSS_MULTIPLIER\n",
    "  \n",
    "  assert len(result.shape) == 0, f\"Result should be scalar, but got: {result.shape}\"\n",
    "  return result\n",
    "\n",
    "def network_use_final_sigmoid():\n",
    "  return LOSS_FUNCTION.value[0] == LossType.BINARY_CROSS_ENTROPY.value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yg2ZQKQhIGyn"
   },
   "outputs": [],
   "source": [
    "def loss_func_single(x, y):\n",
    "  assert x.shape == y.shape\n",
    "  assert x.shape == (N_CLASSES,IM_SZ, IM_SZ)\n",
    "  x = tensor(x, dtype=torch.float32).to(device=DEVICE)\n",
    "  y = tensor(y, dtype=torch.float32).to(device=DEVICE)\n",
    "  return loss_func(x[None,:], y[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_-New2KVW8J",
    "outputId": "181c7027-43b0-465c-e8fc-a4f154c58399"
   },
   "outputs": [],
   "source": [
    "network_use_final_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXfA0EH8SvJF"
   },
   "source": [
    "#### Adding a sigmoid in the end of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txbfU41_SvJG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fastai\n",
    "\n",
    "def change_final_layer_to_return_sigmoid_if_applies(model, debug=False):\n",
    "  if network_use_final_sigmoid():\n",
    "    print(\"Adding a sigmoid to the final layer in the network for loss\", LOSS_FUNCTION.name)\n",
    "  else:\n",
    "    print(\"Not using a final sigmoid layer in the network for loss\", LOSS_FUNCTION.name)\n",
    "    return\n",
    "\n",
    "  convLayer = None\n",
    "  idx = None\n",
    "  \n",
    "  for i, layer in enumerate(model.layers[::-1]):\n",
    "    # find last layer, which isn't a tensorbase\n",
    "    if type(layer) == ToTensorBase:\n",
    "      continue\n",
    "    \n",
    "    assert type(layer) == ConvLayer, f\"Found {type(layer)}. Changing of activation function of last computation layer is only supported for conv layers\"\n",
    "\n",
    "    convLayer = layer\n",
    "    idx = len(model.layers) - 1 - i\n",
    "    assert model.layers[idx] == layer\n",
    "    break\n",
    "    \n",
    "  convElt = convLayer[0]\n",
    "\n",
    "  if debug:\n",
    "    print(\"convLayer.add_module\", convLayer.add_module)\n",
    "\n",
    "  print(f\"Changing last ConvLayer, idx = {idx}:\", convLayer)\n",
    "\n",
    "  assert type(convElt) == torch.nn.modules.Conv2d\n",
    "  \n",
    "  # https://webcache.googleusercontent.com/search?q=cache:3AvcftW0JUwJ:https://discuss.pytorch.org/t/append-for-nn-sequential-or-directly-converting-nn-modulelist-to-nn-sequential/7104+&cd=1&hl=de&ct=clnk&gl=de&client=firefox-b-d\n",
    "  convLayer.add_module(str(len(convLayer)+1), nn.Sigmoid())\n",
    "  \n",
    "  print(\"Changed to:\", model.layers[idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxwPLq-8SvJG"
   },
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MaVEaBRSvJG",
    "outputId": "2ec5c26f-dc35-43cd-e0a2-96b455c6fe91",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn = unet_learner(\n",
    "  dls, resnet34,\n",
    "  n_out = N_CLASSES,\n",
    "  loss_func = loss_func,\n",
    "  pretrained=True,\n",
    "  lr = 3e-4,\n",
    "  wd = 1e-4,\n",
    "  # normalize = true ?\n",
    "  # try dropout instead?\n",
    "  cbs = ([SaveModelCallback(fname=\"test-end\", at_end=True), SaveModelCallback(fname=\"test-valid-best\")] if N >= 100 else []) +\n",
    "    [TerminateOnNaNCallback()]\n",
    ")\n",
    "change_final_layer_to_return_sigmoid_if_applies(learn.model)\n",
    "\n",
    "# todo. how might we make this a fully convolutional pipeline?\n",
    "\n",
    "if False:\n",
    "  unet_learner(\n",
    "    dls, models.alexnet,\n",
    "    normalize=True, n_out=len(category_ids),\n",
    "    pretrained=True, config=None,\n",
    "    loss_func=None, opt_func=Adam,\n",
    "    lr=0.001, metrics=None,\n",
    "    model_dir='models', train_bn=True,\n",
    "    blur=False, blur_final=True,\n",
    "    self_attention=False, y_range=None, last_cross=True,\n",
    "    bottle=False, norm_type=None\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qp06mG_6XwBP",
    "outputId": "96fcd5c3-adf3-4a31-bbea-a382e2eb2da7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(learn.model,learn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Learning Rate\n",
    "\n",
    "See [Fastai lr_find()](https://docs.fast.ai/callback.schedule.html#Learner.lr_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \" √ó \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s ‚Üí %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#del learn\n",
    "#torch.cuda.empty_cache()\n",
    "#dump_tensors(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find(start_lr=10e-7, end_lr=10e-1) #\n",
    "# wd=1e-1, SuggestedLRs(valley=0.00014454397023655474)\n",
    "# wd = 1e-2, SuggestedLRs(valley=0.00010964782268274575)\n",
    "# wd = 1e-3, 3e-4\n",
    "# wd = 1e-4, 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j30t1rFXSvJG"
   },
   "source": [
    "### Visualize Batches and Multi-Label Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaBQJlA4SvJG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "from scipy.ndimage.measurements import center_of_mass\n",
    "from skimage import measure\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "matplotlib_colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "                      'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "                      'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "\n",
    "class_colormaps = (matplotlib_colormaps * 3)[:N_CLASSES]\n",
    "\n",
    "if \"add transparenc to default matplotlib colors\":\n",
    "  # ref: https://stackoverflow.com/questions/37327308/add-alpha-to-an-existing-matplotlib-colormap\n",
    "  class_colormaps = [ cmap(np.arange(cmap.N)) for name in class_colormaps for cmap in [plt.get_cmap(name)] ]\n",
    "  for i in range(len(class_colormaps)):\n",
    "    cmap = class_colormaps[i]\n",
    "    cmap[:,-1] = np.linspace(0, 1, cmap.shape[0]).clip(0,1)\n",
    "    class_colormaps[i] = ListedColormap(cmap)\n",
    "    # print(class_colormaps[i])\n",
    "\n",
    "BOUNDARY_SIZE = 6\n",
    "SOFT_FILL_FACTOR = 0.05\n",
    "\n",
    "def show_multi_segmentation(ax, seg, alpha = 0.75, title_suffix=\"\"):\n",
    "  \"Show a multi-label segmentation ontop of the existing figure where the image is shown.\"\n",
    "  \n",
    "  seg = np.array(seg.cpu(), dtype=SEG_DTYPE)\n",
    "  nDefined = len([0 for i in range(N_CLASSES) if np.any(seg[i] != 0)])\n",
    "  \n",
    "  ax.set_title(f\"{nDefined} classes{title_suffix}\", size=\"large\")\n",
    "  \n",
    "  kernel = np.ones((BOUNDARY_SIZE*2+1,BOUNDARY_SIZE*2+1), dtype=np.int16)\n",
    "\n",
    "  assert seg.shape[0] == N_CLASSES\n",
    "  assert seg.shape[1] == seg.shape[2]\n",
    "  \n",
    "  for i in range(N_CLASSES):\n",
    "    is_defined_for_class = np.any(seg[i] != 0)\n",
    "    \n",
    "    if is_defined_for_class:\n",
    "      boundary = np.minimum(binary_dilation(seg[i] == 0, kernel),seg[i])\n",
    "      boundaryWithSoftFill = (boundary + seg[i]*SOFT_FILL_FACTOR).clip(0,1)\n",
    "      \n",
    "      res = ax.imshow(boundaryWithSoftFill, cmap=class_colormaps[i], alpha=alpha, label=category_names[i])\n",
    "      \n",
    "      components, nComps = measure.label(boundaryWithSoftFill, return_num=True)\n",
    "      centers = center_of_mass(boundaryWithSoftFill, components, index=list(range(1, nComps+1)))\n",
    "      for (x,y) in centers:\n",
    "        ax.text(y, x, category_names[i],\n",
    "          c=class_colormaps[i].colors[-1],\n",
    "          alpha=1-(1-alpha)/2,\n",
    "          size=\"medium\", horizontalalignment=\"center\",\n",
    "          path_effects=[PathEffects.withStroke(linewidth=3.5, foreground='w')],\n",
    "          bbox=dict(fill=True, facecolor=(1,1,1,0.1), linewidth=0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBq7ggqASvJH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SHOW_FIG_SIZE = 15\n",
    "\n",
    "def show_image(ax, img):\n",
    "  img = Resize(*resize_args)(img) # reizse before display\n",
    "  ax.imshow(img.cpu().permute((1,2,0)))\n",
    "  ax.axis('off')\n",
    "\n",
    "def show_images_with_segmentations(images, *segsList, name=None, gtsIdx=None):\n",
    "  \n",
    "  n = len(images)\n",
    "  \n",
    "  print(f\"============================ {name}, n={n} ============================\")\n",
    "  \n",
    "  for segsIdx, segs in enumerate(segsList):\n",
    "    fig, axes = plt.subplots(1,n)\n",
    "    fig.set_figheight(SHOW_FIG_SIZE)\n",
    "    fig.set_figwidth(SHOW_FIG_SIZE)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i in range(n):\n",
    "      img, seg = images[i], segs[i]\n",
    "      assert img.shape[0] == 3 and img.shape[1] == img.shape[2], img.shape\n",
    "      assert seg.shape == (N_CLASSES, IM_SZ, IM_SZ), seg.shape\n",
    "      \n",
    "      title_suffix = f\",seg={segsIdx}\"\n",
    "      \n",
    "      if gtsIdx is not None:\n",
    "        gt = segsList[gtsIdx][i]\n",
    "        loss = loss_func_single(seg, gt)\n",
    "        title_suffix = f\"{title_suffix},loss={loss:.05f}\"\n",
    "        \n",
    "      show_image(axes[i], img)\n",
    "      show_multi_segmentation(axes[i], seg, title_suffix=title_suffix)\n",
    "    \n",
    "def convert_processed_images_to_natural_original_images(dl, preprocessed_images):\n",
    "  originals = dl.after_batch.decode(preprocessed_images)\n",
    "  #resized = [ Resize(*resize_args)(im) for im in originals ]\n",
    "  #torch.stack(resized)\n",
    "  return originals\n",
    "    \n",
    "def show_batch(name):\n",
    "  dl = getattr(dls, name)\n",
    "  images, segs = dl.one_batch()\n",
    "  natural_images = convert_processed_images_to_natural_original_images(dl, images)\n",
    "  show_images_with_segmentations(natural_images, segs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "bSqlu_YZSvJH",
    "outputId": "b6c652b2-e51c-44db-9064-778132d1c8b4"
   },
   "outputs": [],
   "source": [
    "show_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "FblbVfjISvJH",
    "outputId": "a342e898-8c2d-42ee-e5af-ba26f5a01437",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_batch(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wne6z9O5SvJH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stats(arrname):\n",
    "  arr = tensor(eval(arrname))\n",
    "  print(arrname + \":\", arr.shape, arr.dtype, arr.max(), arr.min())\n",
    "\n",
    "if not \"debug, difference between train and train_ds in fastai data loader\":\n",
    "\n",
    "  a = dls.train_ds[0][0]\n",
    "  b = IMAGE_ITEM_TFMS(dls.train_ds[0][0])\n",
    "\n",
    "  stats(\"a\")\n",
    "  stats(\"b\")\n",
    "\n",
    "  c = dls.train.one_batch()[0][0]\n",
    "  d = IMAGE_ITEM_TFMS(dls.train.one_batch()[0][0])\n",
    "  stats(\"c\")\n",
    "  stats(\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eHNxceuSvJI"
   },
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W7N_oDiXSvJI",
    "outputId": "2f791b26-d560-4fc7-9ed3-0bc35e91d5d3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FORCE_TRAIN=False\n",
    "\n",
    "# todo. rename and cleanup keys and refactor new parameters generation\n",
    "m00 = \"m00-resnet34-epoch5\"\n",
    "m01 = \"m01-resnet34-epoch700\"\n",
    "m02 = \"m02resnet34-epoch30-n1000\"\n",
    "m03 = \"m02-resnet34-epoch20-n5545\"\n",
    "m04 = \"m02-resnet34-epoch15-n600-freeze_epochs5\"\n",
    "m04_2 = \"m04_2-resnet34-epoch5-n600-freeze_epochs15\"\n",
    "m05 = \"m05-resnet50-epoch1000-n10-freeze_epochs0\"\n",
    "m05_2 = \"m05_2-resnet50-epoch1500-n4-freeze_epochs0\"\n",
    "m07 = \"m07-resnet34-epoch1400-n10-freeze_epochs0-lossBCELoss\"\n",
    "m08 = \"m08-resnet34-epoch20-n1000-freeze_epochs0-lossBCELoss\"\n",
    "m09 = \"m09-resnet34-epoch30-n5545-freeze_epochs0-lossBCELoss\"\n",
    "m09end = \"m09-resnet34-epoch30-n5545-freeze_epochs0-lossBCELoss-model-end\"\n",
    "m10 = \"m10-resnet34-epoch30-n5545-freeze_epochs0-lossBCELossLogits-wd1e-4-lr3e-4\"\n",
    "m10end = \"m10-resnet34-epoch30-n5545-freeze_epochs0-lossBCELossLogits-wd1e-4-lr3e-4-model-end\"\n",
    "#m06 = \"m06-resnet50-epoch20-n5545-freeze_epochs0\"\n",
    "mcurr = \"current\" # only for temporary usage. not stored in storage\n",
    "\n",
    "modelToTrain = m10\n",
    "\n",
    "if modelToTrain in storage_list_artifacts() and not FORCE_TRAIN and modelToTrain != mcurr:\n",
    "  print(f\"Found model {modelToTrain} in sotrage artifacts. Downloading model...\")\n",
    "  storage_retrieve_artifact_by_name(modelToTrain)\n",
    "  learn = learn.load(modelToTrain)\n",
    "  print(\"Done.\")\n",
    "else:\n",
    "  # currently experiemented with temporary training procedure\n",
    "  if modelToTrain == mcurr:\n",
    "    learn.fine_tune(200, freeze_epochs=0)\n",
    "  \n",
    "  elif modelToTrain == m00:\n",
    "    learn.fine_tune(5, freeze_epochs=1)\n",
    "    \n",
    "  elif modelToTrain == m01:\n",
    "    learn.fine_tune(700, freeze_epochs=1)\n",
    "    \n",
    "  elif modelToTrain == m02:\n",
    "    learn.fine_tune(30, freeze_epochs=1)\n",
    "    \n",
    "  elif modelToTrain == m03:\n",
    "    learn.fine_tune(20, freeze_epochs=1)\n",
    "    \n",
    "  elif modelToTrain == m04:\n",
    "    learn.fine_tune(15, freeze_epochs=5)\n",
    "    \n",
    "  elif modelToTrain == m04_2:\n",
    "    learn.fine_tune(5, freeze_epochs=15)\n",
    "    \n",
    "  elif modelToTrain == m05:\n",
    "    learn.fine_tune(1000, freeze_epochs=0)\n",
    "    \n",
    "  elif modelToTrain == m05_2:\n",
    "    learn.fine_tune(1500, freeze_epochs=0)\n",
    "    \n",
    "  elif modelToTrain == m07:\n",
    "    learn.fine_tune(700, freeze_epochs=0)\n",
    "    \n",
    "  elif modelToTrain == m08:\n",
    "    learn.fine_tune(20, freeze_epochs=0)\n",
    "    \n",
    "  elif modelToTrain == m09:\n",
    "    learn.fine_tune(30, freeze_epochs=0)\n",
    "    \n",
    "  elif modelToTrain == m10:\n",
    "    learn.fine_tune(15, freeze_epochs=0)\n",
    "    \n",
    "  else:\n",
    "    # learn.show_training_loop()\n",
    "    # learn.fine_tune(10)\n",
    "    # learn.fit_one_cycle(5)\n",
    "    raise NotImplementedError\n",
    "\n",
    "  learn.save(modelToTrain)\n",
    "  if modelToTrain != mcurr:\n",
    "    print(\"Finished training. Saving model as storage artifact.\")\n",
    "    storage_store_artifact_by_name(Path(\"curr\") / \"models\" / (modelToTrain + \".pth\"), modelToTrain, force=True)\n",
    "\n",
    "  # TODO. write code, that makes it easier to load and use the end and valid-best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelToTrain + \"end\" in globals():\n",
    "  storage_store_artifact_by_name(Path(\"curr\") / \"models\" / (\"test-valid-end.pth\"), globals()[modelToTrain+\"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqMD5rg7SvJI"
   },
   "outputs": [],
   "source": [
    "# TODOs.\n",
    "# * augmentation\n",
    "# * loss curve plotting and saving\n",
    "# * experiemnt around with learning rate - and use lr-finder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cgzw4jmYSvJI",
    "outputId": "70682784-9b5b-484f-cc05-99d3f05f3078"
   },
   "outputs": [],
   "source": [
    "storage_list_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHm78whDSvJI",
    "outputId": "ec7d0881-f420-4bde-c97a-5604a32e5492"
   },
   "outputs": [],
   "source": [
    "ls(\"curr/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ifxl2F7jvlri",
    "outputId": "ba1540b5-abf1-4fd5-9ff2-bd821afda7dc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKID3BxYSvJJ"
   },
   "outputs": [],
   "source": [
    "# results of a debugging session on the loss\n",
    "\n",
    "if not \"demonstrate\":\n",
    "  ones = torch.ones((1,5,2,2), dtype=torch.float32).to(device=DEVICE)\n",
    "\n",
    "  halfs = ones/2\n",
    "\n",
    "  zeros = torch.zeros((1,5,2,2), dtype=torch.float32).to(device=DEVICE)\n",
    "\n",
    "  lf_old = torch.nn.BCEWithLogitsLoss(\n",
    "        reduction='mean',\n",
    "        #pos_weight=category_weights.reshape((1,N_CLASSES,1,1)).to(device=DEVICE)\n",
    "          # alt: tensor[np.newaxis,N_CLASSES,np.newaxis,np.newaxis]\n",
    "          # Bug with pytorch in pos_weight: https://discuss.pytorch.org/t/loss-function-for-multi-class-semantic-segmentation/40596?u=ptrblck\n",
    "    )\n",
    "\n",
    "  lf_new = torch.nn.BCELoss(\n",
    "        reduction='mean', # note. no weight balancing here\n",
    "    )\n",
    "\n",
    "  org_weights = tensor([1,2,3,4,5],dtype=torch.float32)\n",
    "  ws = org_weights.reshape((1,5,1,1))\n",
    "\n",
    "  assert torch.all(ws.mean(dim=(0,2,3)) == org_weights)\n",
    "\n",
    "  lf_new_bal = torch.nn.BCELoss(\n",
    "        reduction='mean',\n",
    "        weight=ws.to(device=DEVICE)\n",
    "  )\n",
    "\n",
    "\n",
    "  def print_eval(s):\n",
    "    print(s +\": \", eval(s))\n",
    "\n",
    "  print_eval(\"lf_old(ones,zeros)\")\n",
    "  print_eval(\"lf_old(zeros,zeros)\")\n",
    "  print_eval(\"lf_old(ones,halfs)\")\n",
    "\n",
    "  print_eval(\"lf_new(ones,zeros)\")\n",
    "  print_eval(\"lf_new(zeros,zeros)\")\n",
    "  print_eval(\"lf_new(ones,halfs)\")\n",
    "\n",
    "  print_eval(\"lf_new_bal(ones,zeros)\")\n",
    "  print_eval(\"lf_new_bal(zeros,zeros)\")\n",
    "  print_eval(\"lf_new_bal(ones,halfs)\")\n",
    "\n",
    "  # todo. continue debugging here. the losses here yet do not make sense...\n",
    "\n",
    "  # Next step. Make last activation in neural network a sigmoid OR add utilities, that the neural network\n",
    "  # output is fed through a sigmoid whenever we want to take a look at it's results.\n",
    "\n",
    "  # DECISION: Using last step in neural network as sigmoid and then using BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leOnG3KaQahk"
   },
   "outputs": [],
   "source": [
    "def show_results(name):\n",
    "  dl = getattr(dls, name)\n",
    "  preprocessed_images, gts = dl.one_batch()\n",
    "  natural_images = convert_processed_images_to_natural_original_images(dl, preprocessed_images)\n",
    "  gts = gts.cpu()\n",
    "  \n",
    "  with learn.no_bar():\n",
    "    pred_segs = torch.stack([ learn.predict(im.cpu())[0] for im in natural_images])\n",
    "  \n",
    "  if not network_use_final_sigmoid():\n",
    "    pred_segs = torch.sigmoid(pred_segs)\n",
    "\n",
    "  threshold = 0.5\n",
    "  pred_segs_thr = (pred_segs >= threshold).float()\n",
    "  \n",
    "  show_images_with_segmentations(natural_images, gts, pred_segs_thr, name=name, gtsIdx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "3fMqakyqfxNX",
    "outputId": "3c777352-e23f-45ab-cada-bdee52178c9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_results(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "Uwxf_SKWSvJJ",
    "outputId": "80c54c36-8af8-4108-f56d-0d1a2a5932c7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_results(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "scjhmx6vSvJK",
    "outputId": "27c81790-0aae-4e1a-ebf7-e7a38289b61e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False, \"NOTEBOOK_ENDS_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht-gFR2KbhjX"
   },
   "source": [
    "# Initialize detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFhNKYKLXiYG"
   },
   "outputs": [],
   "source": [
    "_ = setup_logger()\n",
    "\n",
    "register_coco_instances(DatasetLabels.TRAIN, {}, Paths.TRAIN_ANNOTATIONS, Paths.TRAIN_IMAGES_DIR)\n",
    "register_coco_instances(DatasetLabels.VAL, {}, Paths.VAL_ANNOTATIONS, Paths.VAL_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEn4diGaA_ph"
   },
   "source": [
    "# Build your Model üè≠\n",
    "\n",
    "We will use Mask R-CNN to generate the segmentation masks for the food items üåØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYqXect1M4Jz"
   },
   "source": [
    "## Configure detectron2\n",
    "\n",
    "Detectron2 has a variety of Instance Segmentation Models. We will use the zoo model with Mask RCNN + ResNet 50. If you want to try other models, you can find them [here]((https://github.com/facebookresearch/detectron2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_q2fu8wXikF"
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (DatasetLabels.TRAIN,)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 273  # Number of output classes\n",
    "\n",
    "cfg.OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzaHmc0IPAIZ"
   },
   "source": [
    "## Load the pre-trained weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWmZ6VySPC14"
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wg5Ra2qzO5YV"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fj5HPBMVO-I3"
   },
   "outputs": [],
   "source": [
    "cfg.SOLVER.BASE_LR = 0.00025  # Learning Rate\n",
    "cfg.SOLVER.MAX_ITER = 20000  # MAx Iterations\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EER82dGmQGS9"
   },
   "source": [
    "# Train the model üöÇ\n",
    "\n",
    "We will setup tensorboard to check the performance of the model while it is training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMMmjhlO62Pj"
   },
   "source": [
    "## Setting up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMOcJUcmfQI8"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6E_Np0qn64uM"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZS5IZOx5XjDm"
   },
   "outputs": [],
   "source": [
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GBGSt63QZf1"
   },
   "source": [
    "# Evaluating the model üß™\n",
    "\n",
    "We will check the performance of our model on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC0EzphvkslK"
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (DatasetLabels.VAL, )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5VkmUr1Qlyj"
   },
   "source": [
    "## Generate predictions on validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrfk7SynXjbV"
   },
   "outputs": [],
   "source": [
    "evaluator = COCOEvaluator(DatasetLabels.VAL, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "data_loader = build_detection_test_loader(cfg, DatasetLabels.VAL)\n",
    "results = inference_on_dataset(predictor.model, data_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNb_H5L-xX6d"
   },
   "source": [
    "## Visualizing the results üëì\n",
    "\n",
    "Numbers are good, but visualizations are better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Gr_8OPtfyja"
   },
   "outputs": [],
   "source": [
    "metadata = MetadataCatalog.get(DatasetLabels.VAL)\n",
    "\n",
    "# Load the training annotations if not loaded\n",
    "if not validation_annotations:\n",
    "  with open(Paths.VAL_ANNOTATIONS) as json_file:\n",
    "      annotations = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eXHZzn0S4Dp"
   },
   "source": [
    "## Check the predictions\n",
    "\n",
    "**Note:** If you are not able to see segmentation masks on the images, that generally means that the model didn't predict a mask for that image. You can verify this by doing\n",
    "\n",
    "```python\n",
    "predictions = predictor(img)\n",
    "print(predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1XMJcWLS7rX"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 180\n",
    "\n",
    "\n",
    "# Visualize some random images\n",
    "for i in range(8):\n",
    "  image_filename = np.random.choice(validation_annotations.get(\"images\")).get(\"file_name\")\n",
    "  image_filename = os.path.join(Paths.VAL_IMAGES_DIR, image_filename)\n",
    "\n",
    "  img = cv2.imread(image_filename)\n",
    "  predictions = predictor(img)\n",
    "\n",
    "  v = Visualizer(img[:, :, ::-1],\n",
    "    metadata=metadata, \n",
    "    scale=0.5, \n",
    "    # instance_mode=ColorMode.IMAGE_BW\n",
    "  )\n",
    "  annotated_image = v.draw_instance_predictions(predictions[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "  plt.subplot(2, 4, i+1)\n",
    "  plt.axis('off')\n",
    "  plt.imshow(annotated_image.get_image())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFAos6c6DGcx"
   },
   "source": [
    "# A note on class ID mappings\n",
    "\n",
    "Here is how the category object looks like\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": 2578,\n",
    "  \"name\": \"water\",\n",
    "  \"name_readable\": \"Water\",\n",
    "  \"supercategory\": \"food\"\n",
    "}\n",
    "```\n",
    "\n",
    "Detectron2 usually maps the category IDs to contiguous numbers. For example, consider the following categories,\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id\": 2578,\n",
    "    \"name\": \"water\",\n",
    "    \"name_readable\": \"Water\",\n",
    "    \"supercategory\": \"food\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 1157,\n",
    "    \"name\": \"pear\",\n",
    "    \"name_readable\": \"Pear\",\n",
    "    \"supercategory\": \"food\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2022,\n",
    "    \"name\": \"egg\",\n",
    "    \"name_readable\": \"Egg\",\n",
    "    \"supercategory\": \"food\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Detectron internally maps these categories to something like\n",
    "\n",
    "```json\n",
    "{\n",
    "  0: 2578, # detectron_id: actual_class_id\n",
    "  1: 1157,\n",
    "  2: 2022\n",
    "}\n",
    "```\n",
    "\n",
    "So, when your model detects water, the prediction class ID that your model returns will be `0` and **not** `2578` . You should make sure to map these detectron IDs to their original actual class IDs for your submission to get scored properly.\n",
    "\n",
    "Here's how you can get this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O98OLrwV7IPO"
   },
   "outputs": [],
   "source": [
    "coco_api = COCO(Paths.TRAIN_ANNOTATIONS)\n",
    "\n",
    "category_ids = sorted(coco_api.getCatIds())\n",
    "categories = coco_api.loadCats(category_ids)\n",
    "\n",
    "class_to_category = { int(class_id): int(category_id) for class_id, category_id in enumerate(category_ids) }\n",
    "\n",
    "with open(\"class_to_category.json\", \"w\") as fp:\n",
    "  json.dump(class_to_category, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fThCV_4uAm4"
   },
   "source": [
    "# Ready? Submit to AIcrowd üöÄ\n",
    "\n",
    "Now you can submit the trained model to AIcrowd!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcaSuCpiMVvb"
   },
   "source": [
    "## Submission configuration ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEPtkV57KC54"
   },
   "outputs": [],
   "source": [
    "aicrowd_submission = {\n",
    "    \"author\": \"<your name>\",\n",
    "    \"username\": \"<your aicrowd username>\",\n",
    "    \"description\": \"initial submission with detectron\",\n",
    "    \"debug\": False,\n",
    "    \"model_path\": \"outputs/model_final.pth\",\n",
    "    \"model_type\": \"model_zoo\",\n",
    "    \"model_config_file\": \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\",\n",
    "    \"detectron_model_config\": {\n",
    "      \"ROI_HEADS\": {\n",
    "        \"SCORE_THRESH_TEST\": 0.5,\n",
    "        \"NUM_CLASSES\": 273\n",
    "      }\n",
    "    }\n",
    "}\n",
    "\n",
    "aicrowd_submission[\"description\"] = aicrowd_submission[\"description\"].replace(\" \", \"-\")\n",
    "with open(\"aicrowd.json\", \"w\") as fp:\n",
    "  json.dump(aicrowd_submission, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PKKwhlkKCfc"
   },
   "source": [
    "## Submit to AIcrowd\n",
    "\n",
    "**Note:** We will create an SSH key on your google drive. This key will be used to identify you on gitlab.aicrowd.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFB2X_nkrGU5"
   },
   "outputs": [],
   "source": [
    "!bash <(curl -sL https://gitlab.aicrowd.com/jyotish/food-recognition-challenge-detectron2-baseline/raw/master/utils/submit-colab.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrEBfu7rPHir"
   },
   "source": [
    "# üìé Important Links\n",
    "\n",
    "* üí™ Challenge Page: https://www.aicrowd.com/challenges/food-recognition-challenge\n",
    "* üó£ Discussion Forum: https://discourse.aicrowd.com/c/food-recognition-challenge\n",
    "* üèÜ Leaderboard: https://www.aicrowd.com/challenges/food-recognition-challenge/leaderboards  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Detectron2 Starter Kit",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07dc319d05a04d16ae9925c649f09ff1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d32ceb4ff484eafaa599eea9f682366": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e0ec7d3fb2c40dd953992e3ffe94dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6005fbc4e6be42c097cae12d6c03fafc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_24da74bd0f5047b997880eb274e13bdd",
      "value": "100%"
     }
    },
    "12092f96182f48ccb6a0699823f35b6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e0ec7d3fb2c40dd953992e3ffe94dc4",
       "IPY_MODEL_e3f28e3ba2ec434bad2a98919e127f65",
       "IPY_MODEL_1e7b97148f414244b351ea2cf1f93565"
      ],
      "layout": "IPY_MODEL_85ce7cdcc53544518a51a31bff93c81e"
     }
    },
    "1e7b97148f414244b351ea2cf1f93565": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_999982caa21d4f6c825f778edfa073d5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_571d4530eb7748968c892eb0a65c1aaa",
      "value": " 5545/5545 [00:21&lt;00:00, 320.06it/s]"
     }
    },
    "24da74bd0f5047b997880eb274e13bdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28158f33ed2b4af49f8c2796a13d4c95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0bd818e72d4898b44f50c3446dee38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fdf11aad98b4862950ac00a6d29e73e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f75519591d244f69e857c870ca2fe84",
       "IPY_MODEL_e5f280703daf409d850e371fda8204c0",
       "IPY_MODEL_9ce9f7c87d3140748f15c11851547f04"
      ],
      "layout": "IPY_MODEL_aab9d20dd4604a71a37b3c74626cb353"
     }
    },
    "3a4db9db30bf43ee875bbd16a7b15732": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ae740682d564544a849983cdf18f4f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "447d90aef0ab49b8a0eabe9a77264b55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "455ef386d22c4544a6156aadb97f46c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54488d6fc9234ec7aa95837b7c645378": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5611b8eb887c4d92a68c7cc484f9aaa4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "571d4530eb7748968c892eb0a65c1aaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6005fbc4e6be42c097cae12d6c03fafc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f75519591d244f69e857c870ca2fe84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e38318e2b9554cc48538095838f6f240",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2c0bd818e72d4898b44f50c3446dee38",
      "value": "100%"
     }
    },
    "770da34f27954335889fedcc42d91582": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85ce7cdcc53544518a51a31bff93c81e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ef6530ba41d4ea6843b7446cb869a03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5b54b0ac00a422db1bac3fc2dd5a171",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d56c205969074c2b949491975071a9ff",
      "value": "100%"
     }
    },
    "999982caa21d4f6c825f778edfa073d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c6e6f779f484d98937e23c7b0e166f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d7b2553bb2504f17a54b16b75ec29080",
       "IPY_MODEL_c1aff1a21d4f40158d5bba895058a107",
       "IPY_MODEL_f4bdecbf9b9f45e3b33e5d4877cb3925"
      ],
      "layout": "IPY_MODEL_770da34f27954335889fedcc42d91582"
     }
    },
    "9ce9f7c87d3140748f15c11851547f04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28158f33ed2b4af49f8c2796a13d4c95",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_07dc319d05a04d16ae9925c649f09ff1",
      "value": " 291/291 [00:00&lt;00:00, 582.61it/s]"
     }
    },
    "a16f964148534988844916605e60db5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa0ef12cd9f543538b9ee0b2059e0ac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aab9d20dd4604a71a37b3c74626cb353": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b52424e01f7f46c79e92357bb7c8dc6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1aff1a21d4f40158d5bba895058a107": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5611b8eb887c4d92a68c7cc484f9aaa4",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c83ecb46088041f1ad283b24064be2b9",
      "value": 10
     }
    },
    "c83ecb46088041f1ad283b24064be2b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf2ec2a22bd54d4abcabcfd0dfa2f33f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ef6530ba41d4ea6843b7446cb869a03",
       "IPY_MODEL_d90e19e725ea409f958c67b9cb5581bd",
       "IPY_MODEL_fc09ee524b6d430ea618dfe20ac8c2e1"
      ],
      "layout": "IPY_MODEL_fbe7be75d1e144deb6ac161d5d2e92ae"
     }
    },
    "d56c205969074c2b949491975071a9ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7b2553bb2504f17a54b16b75ec29080": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_455ef386d22c4544a6156aadb97f46c0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b52424e01f7f46c79e92357bb7c8dc6b",
      "value": "100%"
     }
    },
    "d7d24f6f8aaf4a899fa829ceceb059a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d90e19e725ea409f958c67b9cb5581bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7d24f6f8aaf4a899fa829ceceb059a0",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9c92eab54994e72b9418c02411f2220",
      "value": 2
     }
    },
    "e38318e2b9554cc48538095838f6f240": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3f28e3ba2ec434bad2a98919e127f65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a4db9db30bf43ee875bbd16a7b15732",
      "max": 5545,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aa0ef12cd9f543538b9ee0b2059e0ac3",
      "value": 5545
     }
    },
    "e5f280703daf409d850e371fda8204c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a16f964148534988844916605e60db5d",
      "max": 291,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54488d6fc9234ec7aa95837b7c645378",
      "value": 291
     }
    },
    "e9c92eab54994e72b9418c02411f2220": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4bdecbf9b9f45e3b33e5d4877cb3925": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_447d90aef0ab49b8a0eabe9a77264b55",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0d32ceb4ff484eafaa599eea9f682366",
      "value": " 10/10 [00:06&lt;00:00,  1.98it/s]"
     }
    },
    "f5b54b0ac00a422db1bac3fc2dd5a171": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9ea918c2fad469b8b03ee49768912e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbe7be75d1e144deb6ac161d5d2e92ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc09ee524b6d430ea618dfe20ac8c2e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ae740682d564544a849983cdf18f4f5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f9ea918c2fad469b8b03ee49768912e4",
      "value": " 2/2 [00:00&lt;00:00, 22.94it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
